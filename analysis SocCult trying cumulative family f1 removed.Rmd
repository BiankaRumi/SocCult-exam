---
title: "Analysis SocCult"
output: 
  md_document:
      variant: markdown_github
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

.libPaths( c("C:/Users/biank/Documents/Skole/CogSci/R_packages", .libPaths() ) )
.libPaths()

library(pacman)
pacman::p_load(tidyverse,brms)


```



```{r echo=TRUE}
#read the two datasets
combined_eng <- read_csv("combined_eng.csv")




combined_dan<-read_csv("combined_danish.csv")

unique(combined_eng$ID)
unique(combined_dan$ID)

#we make the danish IDs different so they do not match the english IDs
combined_dan$ID<-combined_dan$ID+100

#merge the two datasets

combined<-merge(combined_dan, combined_eng,all=T)


combined$d_data<-as.numeric(combined$d_data)
combined$q_data<-as.numeric(combined$q_data)
combined$f_data<-as.numeric(combined$f_data)




#make a column for CPI_change

combined$CPI_change<-combined$CPI_current-combined$CPI_native

set.seed(1999)
```


## Template

### Define hypotheses / Describe variables

I will test for the following two hypotheses: 
H1: Corruption rate of one's native country has an effect on interpersonal trust.

H2: The effect declines over time spent in a non-corrupt country like Denmark

Variables involved:

Outcome: 3 different scores of interpersonal trust

- trust_prisoners_dilemma (d_data), 2 rounds times scores 0-1, binomial

- trust_faces (face_data), 20 observation times scores from 1-6
discrete variable, only positive, normally distributed

- trust_questionnaire (q_data), 4 questions scores from 1-6
discrete variable, only positive, normally distributed


Predictors: 

H1:
CPI_native: Corruption Perception Index, a scale from 1-100, where 100 indicates no corruption at all for native country: discrete variable, only positive, 

Condition: dilemma 1 or 2 ; faces 1 - 20; q 1- q4

ID

H2: 
CPI_native
Condition
ID
Years: Number of years spent in current country
CPI_change = CPI_current - CPI_native
            Positive score = Less corruption
            Negative score = more corruption
0 = no change, probably havenâ€™t moved


Other predictors I consider to include to rule out other underlying factors
- Native country GDP
- Highest level of education

I will make the following models:

H1: 

m0: Interpersonal trust ~ condition + (1 + condition | ID)

m1: Interpersonal trust ~ CPI_native + condition,

m2: Interpersonal trust ~ CPI_native + condition  ,

Each of these for the 3 different outcome variables, so 9 models.


H2: 

m3: Interpersonal trust  ~ 1 + CPI_native + CPI_change + condition CPI_change:Years ,
m4: Interpersonal trust  ~ 1 + CPI_native + CPI_change + condition CPI_change:Years + (1 + condition | ID)

Each of these fir the 3 different outcome variables, so 6 models.



################################## H1 ############################### 

```{r}

#First, I will define my models:


f0_d<-bf(d_data ~ 1 + d_condition + (1 + d_condition | ID)) #bernoulli
f0_q<-bf(q_data ~ 1 + q_condition + (1 + q_condition | ID))#cumulative?
f0_f<-bf(f_data ~ 1 + f_condition + (1 + f_condition | ID)) #cumulative




f2_d<-bf(d_data ~ CPI_native + d_condition + (1 + d_condition | ID))
f2_q<-bf(q_data ~ CPI_native + q_condition + (1 + q_condition | ID))
f2_f<-bf(f_data ~ CPI_native + f_condition + (1 + f_condition | ID))


 

```


Making the priors for m0
```{r Priors for the null models}
############### prior for dilemma #####################
hist(combined$d_data)
hist(combined$f_data)
hist(combined$q_data)

get_prior(f0_d, combined, bernoulli())

f0_d_prior<-c(
  prior(normal(0.5, 0.2), class = Intercept),
  prior(normal(0.2, .1), class = b),
  prior(normal(0, .1), class = sd)
)

f0_d_prior_m <- brm(
  f0_d,
  combined,
  family = bernoulli,
  prior = f0_d_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "d_m0_prior"
  )

pp_check(f0_d_prior_m,nsamples=100)




############# f0 prior for questionnaire ###################

get_prior(f0_q, combined, family=cumulative)

f0_q_prior<-c(
  prior(normal(3.5, 1), class = Intercept),
  prior(normal(0.5, .2), class = b),
  prior(normal(0, .1), class = sd)
  )


f0_q_prior_m <- brm(
  f0_q,
  combined,
  family = cumulative(),
  prior = f0_q_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "q_m0_prior"
  )

pp_check(f0_q_prior_m,nsamples=100) # why so many predictions at 1???







###################### f0 prior for faces ##################################

get_prior(f0_f, combined, family=cumulative)

f0_f_prior<-c(
  prior(normal(3.5, 1), class = Intercept),
  prior(normal(1, .3), class = b),
  prior(normal(0, .1), class = sd)
  )


f0_f_prior_m <- brm(
  f0_f,
  combined,
  family=cumulative,
  prior = f0_f_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "f_m0_prior"
  )

pp_check(f0_f_prior_m,nsamples=100) # why so many predictions at 1???




```

```{r Running the null models}


############################## null model for dilemma #################################
d_m0 <- brm(
  f0_d,
  combined,
  family = bernoulli(),
  prior = f0_d_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "d_m0"
  )

summary(d_m0)


pp_check(d_m0,  nsamples=100)



################################ null model for questionnaire ###################################

q_m0 <- brm(
  f0_q,
  combined,
  family = cumulative,
  prior = f0_q_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "q_m0"
  )

summary(q_m0)

pp_check(q_m0, nsamples=100)



############################### null model for faces######################################


f_m0 <- brm(
  f0_f,
  combined,
  family=cumulative(),
  prior = f0_f_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "f_m0"
  )

pp_check(f_m0,nsamples=100)
summary(f_m0)


#pp_check - the data seems to have learned a lot - maybe too much? how to know?

```


```{r Correlation test, see if the different tasks correlate within participants}

#extract the varying effects for dilemma model
d<-ranef(d_m0)
d<-d$ID
d<-as.data.frame(d)
names(d)[names(d)=="Estimate.Intercept"]<-"D_var_ef"


#extract th varying effects for questionnaire model
q<-ranef(q_m0)
q<-q$ID
q<-as.data.frame(q)

colnames(q)
#I average the variation for the different conditions within a condition
q<-q%>%
  select(Estimate.Intercept,Estimate.q_conditionQ2, Estimate.q_conditionQ3, Estimate.q_conditionQ4)
q$Q_var_ef<-apply(q, 1, mean)





#extract varying effects for faces model
f<-ranef(f_m0)
f<-f$ID
f<-as.data.frame(f)

#I average the variation for the different conditions within a condition
colnames(f)


f<-f[ , grepl( "Estimate" , names( f ) ) ]

f$F_var_ef<-apply(f, 1, mean)



#make them one df
var_ef<-cbind(f$F_var_ef,q$Q_var_ef,d$D_var_ef) # V1 = faces, V2 = questionnaire, V3 = dilemma


#we make a model to see the rescore value which indicates the correlation.
f_cor <- 
  brm(data = var_ef, 
      family = gaussian,
      mvbind(V1,V2,V3) ~ 1,
      iter = 2000, warmup = 500, chains = 4, cores = 4, 
      seed = 210191) #rhat of 3 + , model has not converged - why ? 

summary(f_cor) #Faces and questionnaire are somehow correlated (0.53) but not the others
```






```{r Prior for dilemma, m2}
####### f2 prior for dilemma ############
get_prior(f2_d, combined, family = bernoulli())

f2_d_prior<-c(
  prior(normal(0.5, 0.2), class = Intercept),
  prior(normal(0.2, .1), class = b, coef=CPI_native),
  prior(normal(0.2, .1), class = b, coef=d_conditionDilemma2),
  prior(normal(0, .1), class = sd)
)

f2_d_prior_m <- brm(
  f2_d,
  combined,
  family = bernoulli(),
  prior = f2_d_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "d_m2_prior"
  )


pp_check(f2_d_prior_m,nsamples=100)





```


```{r Prisoners: Model 2 and model comparison}

################## making the models #####################





d_m2 <- brm(
  f2_d,
  combined,
  family = bernoulli,
  prior = f2_d_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "d_m2"
  )

summary(d_m2) # no seemingly credible effect of CPI_native

pp_check(d_m2,  nsamples=100)

d_m0<-add_criterion(d_m0,criterion="loo")

d_m2<-add_criterion(d_m2,criterion="loo")

loo_compare(d_m0, d_m2)
loo_model_weights(d_m0, d_m2) # the null model is best to capture the variance in the data with a loo weight of 0.866



```


```{r Priors for Questionnaire, m2}


####################### f2 prior for questionnaire ####################

get_prior(f2_q, combined, family=cumulative)

f2_q_prior<-c(
  prior(normal(3.5, 1), class = Intercept),
  prior(normal(0.3, .1), class = b),
  prior(normal(0.3, .1), class = b,coef=CPI_native),
  prior(normal(0.3, .1), class = b,coef=q_conditionQ2),
  prior(normal(0.3, .1), class = b,coef=q_conditionQ3),
  prior(normal(0.3, .1), class = b,coef=q_conditionQ4),
  prior(normal(0, .1), class = sd)
  )


f2_q_prior_m <- brm(
  f2_q,
  combined,
  family=cumulative,
  prior = f2_q_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "q_m2_prior"
  )

pp_check(f2_q_prior_m,nsamples=100) # still many predictions at 1....





```

```{r Questionnaire model and comparisons }



q_m2 <- brm(
  f2_q,
  combined,
  family=cumulative,
  prior = f2_q_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "q_m2"
  )

summary(q_m2) # a small effect of CPI_native, 0.02

pp_check(q_m2, nsamples=100)

q_m0<-add_criterion(q_m0,criterion="loo")

q_m2<-add_criterion(q_m2,criterion="loo")

loo_compare(q_m0, q_m2)
loo_model_weights(q_m0, q_m2)#  however, the null model performs the best which makes it seems like there is no evidence for the hypothesis
```



```{r Faces, prior for model 2 }

####################### f2 prior for faces ############################


get_prior(f2_f, combined, cumulative())
mean(combined$f_data,na.rm=TRUE)
sd(combined$f_data,na.rm=TRUE)

f2_f_prior<-c(
  prior(normal(3.5, 1), class = Intercept),
  prior(normal(0.5, .3), class = b),
  prior(normal(0.3, .1), class = b, coef=CPI_native),
  prior(normal(0, .1), class = sd)
  )


f2_f_prior_m <- brm(
  f2_f,
  combined,
  family = cumulative,
  prior = f2_f_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "f_m2_prior"
  )

pp_check(f2_f_prior_m,nsamples=100)







```


```{r Faces Model 2 and comparisons}

########################## making the models #################################




f_m2 <- brm(
  f2_f,
  combined,
  family=cumulative(),
  prior = f2_f_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "f_m2"
  )

pp_check(f_m2,nsamples=100)
summary(f_m2) # no effect of CPI_native


f_m0<-add_criterion(f_m0,criterion="loo")

f_m2<-add_criterion(f_m2,criterion="loo")

loo_compare(f_m0,f_m2) #m2 is the best model with the weight of 1
loo_model_weights(f_m0, f_m2)

```





Visualization of results:
```{r Hypothesis testing H1}




plot(hypothesis(d_m2,"CPI_native > 0")) #no effect
hypothesis(d_m2,"CPI_native > 0")

plot(hypothesis(q_m2,"CPI_native > 0")) #small effect of 0.02 with Inf for evid.ratio, however, this was not the best model in model comparison - the null model performed better
hypothesis(q_m2,"CPI_native > 0")

plot(hypothesis(f_m2,"CPI_native > 0"))
hypothesis(f_m2,"CPI_native > 0") # no effect



```


Model comparisons:
```{r}

```





################################## H2 ############################### 

```{r Models for H2}



f3_d<- bf(d_data ~ 1 + CPI_native + CPI_change + d_condition + CPI_change * Years_Current + (1 + d_condition | ID))

f3_q<- bf(q_data ~ 1 + CPI_native + CPI_change + q_condition + CPI_change * Years_Current + (1 + q_condition | ID))

f3_f<- bf(f_data ~ 1 + CPI_native + CPI_change + f_condition + CPI_change * Years_Current + (1 + f_condition | ID))

```



```{r H2 Priors for Prisoners Dilemma }
############### prior for dilemma #####################

get_prior(f3_d, combined, bernoulli())

f3_d_prior<-c(
  prior(normal(0.5, 0.2), class = Intercept),
  prior(normal(0.2, .1), class = b, coef=CPI_native),
  prior(normal(0.2, .1), class = b, coef=d_conditionDilemma2),
  prior(normal(0.2, .1), class = b, coef=Years_Current),
  prior(normal(0.2, .1), class = b, coef=CPI_change:Years_Current),
  prior(normal(0, .1), class = sd)
)


f3_d_prior_m <- brm(
  f3_d,
  combined,
  family = bernoulli,
  prior = f3_d_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "d_m3_prior"
  )

pp_check(f3_d_prior_m,nsamples=1000) # why so few lines??



```


```{r H2 Prisoners: Models and model comparison}

################## making the models #####################



d_m3 <- brm(
  f3_d,
  combined,
  family = bernoulli(),
  prior = f3_d_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "d_m3"
  )

summary(d_m3) # no credible interaction effect



d_m3<-add_criterion(d_m3,criterion="loo")


loo_compare(d_m0, d_m2, d_m3)
loo_model_weights(d_m0, d_m2, d_m3) #null model still outperforms the other models
```






```{r Priors for Questionnaire}

############# f0 prior for questionnaire ###################

get_prior(f3_q, combined, cumulative())


f3_q_prior<-c(
  prior(normal(3.5, 1), class = Intercept),
  prior(normal(0.3, .1), class = b),
  prior(normal(0.3, .1), class = b,coef=CPI_native),
  prior(normal(0, .1), class = sd),
  prior(normal(0.2, .1), class = b, coef=Years_Current),
  prior(normal(0.2, .1), class = b, coef=CPI_change:Years_Current)
  )


f3_q_prior_m <- brm(
  f3_q,
  combined,
  family = cumulative,
  prior = f3_q_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "q_m3_prior"
  )

pp_check(f3_q_prior_m,nsamples=100) #pretty bad predictions again...





```

```{r Questionnaire models and comparisons }

q_m3 <- brm(
  f3_q,
  combined,
  family = cumulative,
  prior = f3_q_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "q_m3"
  )

summary(q_m3) # no interaction effect, but small effects for CPI_current and CPI_change



q_m3<-add_criterion(q_m3,criterion="loo")
q_m0<-add_criterion(q_m0, criterion="loo")

loo_compare(q_m0, q_m2, q_m3) # null model is the best model, 0.854

loo_model_weights(q_m0, q_m2, q_m3)
```



```{r Faces Priors }



####################### f3 prior for faces ############################


get_prior(f3_f, combined, cumulative())

f3_f_prior<-c(
  prior(normal(3.7, 1), class = Intercept),
  prior(normal(0.5, .3), class = b),
  prior(normal(0.3, .1), class = b, coef=CPI_native),
  prior(normal(0.5, .3), class = sd),
  prior(normal(0.2, .1), class = b, coef=Years_Current),
  prior(normal(0.2, .1), class = b, coef=CPI_change:Years_Current)
  )


f3_f_prior_m <- brm(
  f3_f,
  combined,
  family = cumulative,
  prior = f3_f_prior,
  sample_prior = "only",
  chains = 2,
  cores = 3,
  file = "f_m3_prior"
  )

pp_check(f3_f_prior_m,nsamples=100)







```


```{r Faces Models and comparisons}

########################## making the models #################################


f_m3 <- brm(
  f3_f,
  combined,
  family=cumulative(),
  prior = f3_f_prior,
  sample_prior = T,
  chains = 2,
  cores = 3,
  file = "f_m3"
  )

pp_check(f_m3,nsamples=100)

pp_check(q_m3, nsamples=100)
pp_check(d_m3, nsamples=100)
summary(f_m3) # no interaction effect


f_m3<-add_criterion(f_m3,criterion="loo") 


loo_compare(f_m0, f_m2, f_m3)
loo_model_weights(f_m0, f_m2, f_m3) # model 3 is the best one, but why is that when therer is no credible interaction?
```







Visualization of results:
```{r Hypothesis tests H2}

plot(hypothesis(d_m3,"CPI_change:Years_Current > 0"))
hypothesis(d_m3,"CPI_change:Years_Current > 0")


plot(hypothesis(q_m3,"CPI_change:Years_Current > 0"))
hypothesis(q_m3,"CPI_change:Years_Current > 0")


plot(hypothesis(f_m3,"CPI_change:Years_Current > 0"))
hypothesis(f_m3,"CPI_change:Years_Current > 0")




```


